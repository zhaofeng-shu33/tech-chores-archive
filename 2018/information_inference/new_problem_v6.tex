\documentclass{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{cancel}
\usepackage{bm}
\DeclareMathOperator{\diag}{diag}
\def\T{\mathrm{T}}
\def\E{\mathbb{E}}
\def\Var{\textrm{Var}}
\begin{document}
\textbf{Problem}: Suppose $|v(y)^\T s(x)|+|d(y)|\leq \epsilon,Q_{Y|X}$ is a given conditional distribution,$$\tilde{P}_{Y|X}(y|x)=\frac{Q_{Y|X}(y|x) e^{v(y)^\T s(x)+d(y)}}{\sum_{y'\in \mathcal{Y}} Q_{Y|X}(y'|x) e^{v(y')^\T s(x)+d(y')}}$$
Find constraints on $s(x),v(y),d(y)$ such that $D(P_{Y,X}||P_X \tilde{P}_{Y|X})$ is minimized.

we expand the denominator of $\tilde{P}_{Y|X}(y|x)$ first:
\begin{align*}
\sum_{y'\in \mathcal{Y}} Q_{Y|X=x}(y'|x) e^{v(y')^\T s(x)+d(y')}=&
1+\E_{Q_{Y|X=x}}[(v(Y)^T s(x)+d(Y))\\
+&\frac{1}{2}(v(Y)^T s(x)+d(Y))^2]+O(\epsilon^3)
\end{align*}
\begin{align*}
-\log\left[\sum_{y'\in \mathcal{Y}} Q_{Y|X=x}(y'|x) e^{v(y')^\T s(x)+d(y')}\right]=&
-\E_{Q_{Y|X=x}}[(v(Y)^T s(x)+d(Y))] \\
-&\frac{1}{2}\E_{Q_{Y|X=x}}[(v(Y)^T s(x)+d(Y))^2]\\
+&\frac{1}{2}  \E^2_{Q_{Y|X=x}}[(v(Y)^T s(x)+d(Y))]+O(\epsilon^3)
\end{align*}
By Maximum Likelyhood Method, the objective function is $\E_{P_{X,Y}}[\log \tilde{P}_{Y|X}]$, and 
\begin{align*}
\E_{P_{X,Y}}[\log \tilde{P}_{Y|X}]=&\E_{P_{X,Y}}[\log Q_{Y|X}]+ \E_{P_{X,Y}}[v(Y)^T s(X)+d(Y)]+\\%\cancelto{0}{\E[d(Y)]}
+&\E_{P_X}\left[-\log\sum_{y'\in \mathcal{Y}} Q_{Y|X}(y'|x) e^{v(y')^\T s(X)+d(y')}]\right]\\
=& C +  \E_{P_X}\left[\sum_{y\in \mathcal{Y}}(P_{Y|X}(y)-Q_{Y|X}(y))(v(y)^\T s(X)+d(y))\right]+\\
+&\frac{1}{2}\E_{P_X}\left[\E^2_{Q_{Y|X=x}}[(v(Y)^T s(x)+d(Y))]\right]-\\
-&\frac{1}{2}\E_{P_X}\left[\E_{Q_{Y|X=x}}[(v(Y)^T s(x)+d(Y))^2]\right]+ O(\epsilon^3)
\end{align*}

We simplify the above expression in matrix form as follows:

let $n\triangleq |\mathcal{Y}|$ and $s(x)$ is a $k-$dimensional vector,$v_1(Y)=[v_1(1),\dots,v_1(n)]$ is a $k\times n$ matrix.
We assume $\mathcal{Y}=\{1,2,\dots,n\}$,then the objective function (omit high order term)to minimize can be rewritten in matrix notation as
\begin{equation}\label{eq:min}
\sum_{x\in \mathcal{X}} P_X(x)\left(\frac{1}{2}s(x)^\T v_1 \bm{Q}_x v_1^\T s(x) +d_1^\T \bm{Q}_x v_1^\T s(x)+\frac{1}{2}d_1^\T \bm{Q}_x d_1-\Xi_x^T (v_1^\T s(x)+d_1)\right)
\end{equation}
where $\Xi_x(y)=P_{Y|X=x}(y)-Q_{Y|X=x}(y),\bm{Q}_x(i,j)=Q_{Y|X=x}(i)\delta_{ij}-Q_{Y|X=x}(i)Q_{Y|X=x}(j)$.
$\Xi_x$ is a n-dimensional vector and $\bm{Q}_x$ is a $n\times n$ matrix.
Taking the derivative w.r.t $v_1$(matrix) and $d_1$(vector) and make them equal zero gives
\begin{align}
\sum_{x\in \mathcal{X}} P_X(x)\left(ss^\T v_1 \bm{Q}_x+sd_1^\T \bm{Q}_x-s\Xi_x^\T\right) = & 0 \label{eq:joint_pre_1}\\
\sum_{x\in \mathcal{X}} P_X(x)\left(\bm{Q}_x v_1^Ts(x)+ \bm{Q}_x d_1-\Xi_x\right)=& 0\label{eq:joint_pre_2}
\end{align}
where $s=s(x)$.
Notice that in general \eqref{eq:joint_pre_1} contains $k\times n$ equations and \eqref{eq:joint_pre_2} contains $n$ equations.



First we consider $k=1$. The combination of (\ref{eq:joint_pre_1},\ref{eq:joint_pre_2}) gives $2n$ equations with $2n$ unknowns $v(y),d(y),y=1,\dots,n$.

Denote $Q_{X,Y}=P_X Q_{Y|X}$, which is a joint distribution and can be computed.
Then $Q_{X}=P_X,Q_{Y}=\sum_{x\in \mathcal{X}}Q_{X,Y}$ and $Q_{X,Y}=Q_Y Q_{X|Y}$
 then the equation \eqref{eq:joint_pre_1} can be simplified as :
\begin{align}\notag
\E_{Q_{X|Y=\hat{y}}}[s^2(X)]v_1(\hat{y})Q_Y(\hat{y})+&\E_{Q_{X|Y=\hat{y}}}[s(X)]Q_Y(\hat{y})d(\hat{y}) \\
+M_1(\epsilon)+\E_{Q_{X|Y=\hat{y}}}[s(X)]Q_Y(\hat{y})=& \E_{P_{X|Y=\hat{y}}}[s(X)]P_Y(\hat{y})\label{eq:joint_simplify_1}
\end{align}
where $\hat{y}\in \{1,2,\dots,n\}$. and $M_1(\epsilon)$ is
$$
M_1(\epsilon)=-\sum_{x\in \mathcal{X}}P_X(x)\left(s(x)Q_{Y|X=x}(\hat{y})\E_{Q_{Y|X=x}}[v_1(Y)s(x)+d(Y)]\right)
$$

And equation \eqref{eq:joint_pre_2} can be simplified as:
\begin{align}\notag
\E_{Q_{X|Y=\hat{y}}}[s(X)]v_1(\hat{y})Q_Y(\hat{y})+&Q_Y(\hat{y})d(\hat{y}) \\
+M_2(\epsilon)+Q_Y(\hat{y})=& P_Y(\hat{y})\label{eq:joint_simplify_2}
\end{align}
where $M_2(\epsilon)$ is
$$
M_2(\epsilon)=-\sum_{x\in \mathcal{X}}P_X(x)\left(Q_{Y|X=x}(\hat{y})\E_{Q_{Y|X=x}}[v_1(Y)s(x)+d(Y)]\right)
$$
stack $[v(1),\dots,v(n),d(1),\dots,d(n)]$ as a $2n$ dimensional vector $\binom{V}{d}$, the combination of (\ref{eq:joint_pre_1},\ref{eq:joint_pre_2}) gives
the matrix equation :
\begin{equation}
\begin{bmatrix}
\sum_{x\in \mathcal{X}} s^2(x)Q_xP_X(x) & \sum_{x\in \mathcal{X}} s(x)Q_xP_X(x) \\
\sum_{x\in \mathcal{X}} s(x)Q_xP_X(x) & \sum_{x\in \mathcal{X}} Q_xP_X(x)
\end{bmatrix}\binom{V}{d}=\binom{\sum_{x\in \mathcal{X}}s(x)\Xi_x}{\sum_{x\in \mathcal{X}}\Xi_x}
\end{equation}
the $2n\times 2n$ matrix in the above equation is positive definite when $s(x)$ is not a constant. Then we can solve $\binom{V}{d}$ by standard matrix inversion.

Below we optimize $s(x)$ given $v(y),d(y)$(1 dimensional case),\eqref{eq:min} is a quadratic function about $s(x)$ (seperate each sum term) and can be solved
explicitly as:
\begin{equation}
s(x)=\frac{\Xi_x^\T v_1^\T -d_1^\T \bm{Q}_x v_1^\T}{v_1\bm{Q}_xv_1^\T}
\end{equation}

Then we consider $k$ dimensional case, stack $[v(1)^\T,\dots,v(n)^\T]$ as a $kn$ dimensional vector, by the definition of Kronecker matrix product 
the combination of (\ref{eq:joint_pre_1},\ref{eq:joint_pre_2}) gives
the matrix equation :
\begin{equation}
\begin{bmatrix}
\sum_{x\in \mathcal{X}} P_X(x) (s(x)s(x)^\T)\otimes Q_x & \sum_{x\in \mathcal{X}} P_X(x) s(x)\otimes Q_x \\
\sum_{x\in \mathcal{X}} P_X(x) Q_x\otimes s(x)^\T & \sum_{x\in \mathcal{X}} P_X(x)Q_x
\end{bmatrix}\binom{V}{d}=\binom{\sum_{x\in \mathcal{X}}s(x)\Xi_x^\T}{\sum_{x\in \mathcal{X}}\Xi_x}
\end{equation}
Notice that $s(x)\otimes Q_x$ is $kn\times n$ dimensional matrix and its transpose is $Q_x \otimes s(x)^\T$.
$(s(x)s(x)^\T)\otimes Q_x$ is $kn \times kn$ dimensional matrix.
The $kn\times (k+1)n$ matrix in the above equation is positive definite when $s(x)$ is not a constant. Then we can solve $\binom{V}{d}$ by standard matrix inversion.

Below we optimize $s(x)$ given $v(y),d(y)$($k$ dimensional case),\eqref{eq:min} is a quadratic form about $s(x)$ (seperate each sum term) and can be solved
explicitly as:
\begin{equation}
s(x)=(v_1\bm{Q}_xv_1^\T)^{-1}(\Xi_x^\T v_1^\T -d_1^\T \bm{Q}_x v_1^\T)
\end{equation}
\end{document}




