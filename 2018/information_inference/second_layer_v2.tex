\documentclass{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{cancel}
\usepackage{bm}
\DeclareMathOperator{\diag}{diag}
\def\T{\mathrm{T}}
\def\E{\mathbb{E}}
\def\Var{\textrm{Var}}
\begin{document}
\textbf{Problem}: 
Consider a MLP nerual network with one hidden layer. Let the activation function of the hidden layer be a monotonic function $f(\cdot)$. Then the output of the $z$-th hidden node is
\begin{equation}\label{eq:constraint}
s_z(x)=f(w_0^\T(z)t(x)+c_0(z)+w^\T(z)t(x)+c(z)),z=1,\dots,k
\end{equation}
Denote $s=[s_1,\dots,s_k]^\T$, which is a $k \times|\mathcal{X}|$ matrix.

Suppose $|v(y)^\T s(x)|+|d(y)|\leq \epsilon,Q_{Y|X}$ is a given conditional distribution,
$$\tilde{P}_{Y|X}(y|x)=\frac{Q_{Y|X}(y|x) e^{v(y)^\T s(x)+d(y)}}{\sum_{y'\in \mathcal{Y}} Q_{Y|X}(y'|x) e^{v(y')^\T s(x)+d(y')}}$$
With further assumption of $|w(z)^\T t(x)|+|c(z)|<\epsilon$. Given $v(y),d(y)$, which is the parameter of the output layer and input feature $t(x)$. Optimize $w(z),c(z)$ such that $D(P_{Y,X}||P_X \tilde{P}_{Y|X})$ is minimized.

By the previous deduction, we should minimize
\begin{equation}\label{eq:min_total}
L(s)=\sum_{x\in \mathcal{X}} P_X(x)L_x(s)
\end{equation}
where
\begin{equation}\label{eq:min_each}
L_x(s)=\frac{1}{2}s(x)^\T v \bm{W}_x v^\T s(x) +(d^\T \bm{W}_x v^\T-\Xi_x^T v^\T) s(x)+C
\end{equation}
If $s(x)$ is unconstrained, then we only need to solve $|\mathcal{X}|$ quaractic optimization problem, and the optimal $s(x)$ is given by:
\begin{equation}
s^*(x)=(v\bm{W}_xv^\T)^{-1}(\Xi_x^\T v^\T -d^\T \bm{W}_x v^\T)
\end{equation}

However, $s(x)$ is contraint by \eqref{eq:constraint}. Therefore, for each $x$ we measure the difference between optimal $L_x(s^*)$ and $L_x(s)$.
In \eqref{eq:min_total},$L(s)$ can be rewritten as:
\begin{equation}\label{eq:total_sim}
L(s)=L(s^*)+\frac{1}{2}\sum_{x\in\mathcal{X}}P_X(x)(s(x)-s^*(x))^\T v \bm{W}_x v^\T (s(x)-s^*(x))
\end{equation}
for the $z$-th component of $s(x)$, we expand \eqref{eq:constraint} around $c(z)$:
\begin{equation}
s_z(x) = f(\omega_0^\T (z) t(x) + c_0 (z) ) + f'(\omega_0 ^\T (z) t(x) + c_0(z)) (\omega^\T(z) t(x)+c(z))+ o(\epsilon)
\end{equation}

Denote
\begin{align*}
\Lambda(x) \triangleq & \diag[f'(\omega_0^\T (1) t(x) + c_0 (1) ),\dots,f'(\omega_0^\T (k) t(x) + c_0 (k) )],k\times k\text{ dimensional matrix} \\
\tilde{f}(x) \triangleq & \Lambda^{-1}(x)[f(\omega_0^\T (1) t(x) + c_0 (1) ),\dots,f(\omega_0^\T (k) t(x) + c_0 (k) )]^\T,k \text{ dimensional vector} 
\end{align*}
Let $w=[w(1),\dots,w(k)]$, which is $m\times k$ matrix. Then 
$s(x)=\Lambda(x)[w^\T t(x)+c+\tilde{f}(x)]$ is a $k$ dimensional vector. The matrix $w$ and vector $c$ are parameters to be optimized.
Let $\tilde{\bm{W}}_x=\Lambda(x) v\bm{W}_x v^\T \Lambda(x),\tilde{\Xi}_x=\tilde{\bm{W}}_x [\Lambda^{-1}(x)s^*(x)-\tilde{f}(x)]$.

By assumption that $|s(x)^\T v(y)|<\epsilon$, if we omit $o(\epsilon^2)$ term ($\tilde{\bm{W}}_x=O(\epsilon)$), equation \eqref{eq:total_sim} can be simplified to:
\begin{equation}
L(s)=c' + \sum_{x \in \mathcal{X}} P_X(x)\left(\frac{1}{2}t(x)^\T w \tilde{\bm{W}}_x w^\T t(x) + c^\T \tilde{\bm{W}}_x w^\T t(x) + \frac{1}{2} c^\T \tilde{\bm{W}}_x c - \tilde{\Xi}_x^\T (w^\T t(x)+c)\right)
\end{equation}
$c'$ is not depedent on $w,c$.

Similar to the deduction for one layer, stack $[w(1)^\T,\dots,w(k)^\T]$ as a $mk$ dimensional vector, by the definition of Kronecker matrix product we
have
\begin{equation}
\begin{bmatrix}
\sum_{x\in \mathcal{X}} P_X(x) (t(x)t(x)^\T)\otimes \tilde{\bm{W}}_x & \sum_{x\in \mathcal{X}} P_X(x) t(x)\otimes \tilde{\bm{W}}_x \\
\sum_{x\in \mathcal{X}} P_X(x) \tilde{\bm{W}}_x \otimes t(x)^\T & \sum_{x\in \mathcal{X}} P_X(x)\tilde{\bm{W}}_x
\end{bmatrix}\binom{W}{c}=\binom{\sum_{x\in \mathcal{X}}t(x)\tilde{\Xi}_x^\T}{\sum_{x\in \mathcal{X}}\tilde{\Xi}_x}
\end{equation}
Notice that $t(x)\otimes \tilde{\bm{W}}_x$ is $mk\times k$ dimensional matrix and its transpose is $\tilde{\bm{W}}_x \otimes t(x)^\T$.
$(t(x)t(x)^\T)\otimes \tilde{\bm{W}}_x$ is $mk \times mk$ dimensional matrix.

\end{document}






